{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b77301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class IndicatorEncoder(nn.Module):\n",
    "    def __init__(self, indicator_dim, context_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(indicator_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, context_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, indicators):\n",
    "        return self.net(indicators)\n",
    "\n",
    "class ContextualKernel(gpytorch.kernels.Kernel):\n",
    "    def __init__(self, base_kernel, context_dim, param_dim):\n",
    "        super().__init__()\n",
    "        self.base_kernel = base_kernel\n",
    "        self.context_dim = context_dim\n",
    "        self.param_dim = param_dim\n",
    "        self.context_scaling = nn.Linear(context_dim, 1)\n",
    "    \n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        # x1 and x2 are (parameters, context embeddings)\n",
    "        p1, c1 = x1[..., :self.param_dim], x1[..., self.param_dim:]\n",
    "        p2, c2 = x2[..., :self.param_dim], x2[..., self.param_dim:]\n",
    "        \n",
    "        base_k = self.base_kernel(p1, p2, diag=diag, **params)\n",
    "        \n",
    "        \n",
    "        if diag:\n",
    "            # Compute context similarity just between matching points\n",
    "            context_diff = c1 - c2  # [N, context_dim]\n",
    "            context_similarity = torch.exp(-torch.norm(context_diff, dim=-1))  # [N]\n",
    "            \n",
    "            scaling_c1 = self.context_scaling(c1).sigmoid().squeeze(-1)  # [N]\n",
    "            scaling_c2 = self.context_scaling(c2).sigmoid().squeeze(-1)  # [N]\n",
    "            scaling = scaling_c1 * scaling_c2  # [N]\n",
    "            \n",
    "            return base_k * context_similarity * scaling\n",
    "        else:\n",
    "            # Full matrix case\n",
    "            c1_exp = c1.unsqueeze(-2)  # [N, 1, context_dim]\n",
    "            c2_exp = c2.unsqueeze(-3)  # [1, M, context_dim]\n",
    "            context_diff = c1_exp - c2_exp  # [N, M, context_dim]\n",
    "            context_similarity = torch.exp(-torch.norm(context_diff, dim=-1))  # [N, M]\n",
    "            \n",
    "            scaling_c1 = self.context_scaling(c1).sigmoid()  # [N, 1]\n",
    "            scaling_c2 = self.context_scaling(c2).sigmoid()  # [M, 1]\n",
    "            scaling = scaling_c1 * scaling_c2.transpose(-1, -2)  # [N, M]\n",
    "            \n",
    "            return base_k * context_similarity * scaling\n",
    "        \n",
    "\n",
    "class RewardVariationalGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, context_dim):\n",
    "        # Define variational distribution + strategy\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(0)\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        base_kernel = gpytorch.kernels.RBFKernel()\n",
    "        self.covar_module = ContextualKernel(base_kernel, context_dim, param_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x, x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "class NN_VariationalGP_Model(nn.Module):\n",
    "    def __init__(self, indicator_dim, param_dim, context_dim, num_inducing=128):\n",
    "        super().__init__()\n",
    "        self.indicator_encoder = IndicatorEncoder(indicator_dim, context_dim)\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        self.num_inducing = num_inducing\n",
    "        self.gp_model = None  # will create dynamically\n",
    "    \n",
    "    def initialize_gp(self, indicators, parameters):\n",
    "        context_embeddings = self.indicator_encoder(indicators)\n",
    "        train_x = torch.cat([parameters, context_embeddings], dim=-1)\n",
    "        \n",
    "        # Choose random inducing points from training data\n",
    "        rand_idx = torch.randperm(train_x.size(0))[:self.num_inducing]\n",
    "        inducing_points = train_x[rand_idx]\n",
    "        \n",
    "        self.gp_model = RewardVariationalGPModel(inducing_points, context_dim=context_embeddings.shape[-1])\n",
    "    \n",
    "    def forward(self, parameters, indicators):\n",
    "        context_embeddings = self.indicator_encoder(indicators)\n",
    "        test_x = torch.cat([parameters, context_embeddings], dim=-1)\n",
    "        \n",
    "        self.gp_model.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self.likelihood(self.gp_model(test_x))\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13cee32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame(columns=['indicators', 'samples'])\n",
    "\n",
    "indicators = np.arange(0, 10)\n",
    "\n",
    "for indicator in indicators:\n",
    "    parameters = np.arange(0, 1, 0.01)\n",
    "    values = parameters + indicator\n",
    "\n",
    "    samples = np.array([[param, param + indicator] for param in parameters])\n",
    "    \n",
    "    example_df = pd.concat([example_df, pd.DataFrame({'indicators': [[indicator, 10 - indicator]], 'samples': [samples]})], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ecf06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your training data the same way\n",
    "# train_indicators, train_parameters, train_rewards\n",
    "\n",
    "indicator_dim = 2\n",
    "param_dim = 1\n",
    "context_dim = 32\n",
    "\n",
    "# Assume you already have your dataframe\n",
    "# df = pd.read_pickle(\"your_dataframe.pkl\")  # or however you load\n",
    "df = example_df.copy()\n",
    "\n",
    "# Sample training points\n",
    "train_indicators = []\n",
    "train_parameters = []\n",
    "train_rewards = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    indicators = torch.tensor(row['indicators'], dtype=torch.float32)\n",
    "    param_reward_array = row['samples']  # array of shape (N_samples, param_dim + 1)\n",
    "    \n",
    "    for sample in param_reward_array:\n",
    "        param_vec = torch.tensor(sample[:-1], dtype=torch.float32)\n",
    "        reward_val = torch.tensor(sample[-1], dtype=torch.float32)\n",
    "        \n",
    "        train_indicators.append(indicators)\n",
    "        train_parameters.append(param_vec)\n",
    "        train_rewards.append(reward_val)\n",
    "\n",
    "train_indicators = torch.stack(train_indicators)\n",
    "train_parameters = torch.stack(train_parameters)\n",
    "train_rewards = torch.stack(train_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fbd9291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100: Loss = 11.7795\n",
      "Iter 200: Loss = 7.7332\n",
      "Iter 300: Loss = 5.8599\n",
      "Iter 400: Loss = 4.8083\n",
      "Iter 500: Loss = 4.1567\n",
      "Iter 600: Loss = 3.7295\n",
      "Iter 700: Loss = 3.4399\n",
      "Iter 800: Loss = 3.2389\n",
      "Iter 900: Loss = 3.0971\n",
      "Iter 1000: Loss = 2.9954\n"
     ]
    }
   ],
   "source": [
    "# Load your training data the same way\n",
    "# train_indicators, train_parameters, train_rewards\n",
    "\n",
    "model = NN_VariationalGP_Model(indicator_dim, param_dim, context_dim)\n",
    "\n",
    "model.initialize_gp(train_indicators, train_parameters)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model.gp_model, num_data=train_indicators.size(0))\n",
    "\n",
    "model.train()\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    cat = torch.cat([train_parameters, model.indicator_encoder(train_indicators)], dim=-1)\n",
    "    # print(cat)\n",
    "\n",
    "    # output = model.likelihood(model.gp_model(cat))\n",
    "    output = model.gp_model(cat)\n",
    "    loss = -mll(output, train_rewards)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Iter {i + 1}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef91da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given new indicator vector and candidate parameters\n",
    "# new_indicators = torch.tensor([[...]], dtype=torch.float32)\n",
    "# new_parameters = torch.tensor([[...]], dtype=torch.float32)\n",
    "\n",
    "new_indicators = train_indicators\n",
    "new_parameters = train_parameters\n",
    "\n",
    "# cat = torch.cat([train_parameters, model.indicator_encoder(train_indicators)], dim=-1)\n",
    "# predictions = model(cat)\n",
    "predictions = model(new_parameters, new_indicators)\n",
    "pred_mean = predictions.mean\n",
    "pred_var = predictions.variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc9d40e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491, 4.3491,\n",
       "         4.3491]),\n",
       " tensor([2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306, 2.9306,\n",
       "         2.9306]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mean, pred_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
