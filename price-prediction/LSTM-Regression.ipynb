{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dummy_npwarn_decorator_factory():\n",
    "  def npwarn_decorator(x):\n",
    "    return x\n",
    "  return npwarn_decorator\n",
    "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay, but seriously there's a big problem\n",
    "\n",
    "The LSTM is trained on min-max scaled data from 2024-2025.\n",
    "\n",
    "But when we do the RL, the min-max scaling happens from 2025-present.\n",
    "\n",
    "So the min-max scaling isn't even the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2675  \n",
      "Epoch 2/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0871\n",
      "Epoch 3/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0847\n",
      "Epoch 4/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0847 \n",
      "Epoch 5/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0861 \n",
      "Epoch 6/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0844 \n",
      "Epoch 7/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0822 \n",
      "Epoch 8/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0835 \n",
      "Epoch 9/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0868\n",
      "Epoch 10/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0827 \n",
      "Epoch 11/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0821 \n",
      "Epoch 12/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0857 \n",
      "Epoch 13/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0829\n",
      "Epoch 14/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0846 \n",
      "Epoch 15/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0809 \n",
      "Epoch 16/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0809\n",
      "Epoch 17/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0845\n",
      "Epoch 18/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0815 \n",
      "Epoch 19/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0788\n",
      "Epoch 20/20\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0836\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Prediction: [[0.5457504]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "# Features: 1000 samples, each with 10 timesteps and 1 feature\n",
    "# Targets: 1000 regression values\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(1000, 10, 1)\n",
    "y = np.random.rand(1000)\n",
    "\n",
    "# Define the LSTM regression model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(10, 1)),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Example prediction\n",
    "sample_input = np.random.rand(1, 10, 1)  # Single sample with 10 timesteps\n",
    "prediction = model.predict(sample_input)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Fold R²: -0.031944932970606965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Fold R²: -0.008463300280357355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000166414D23E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Fold R²: -0.011218091656060247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000016643772DE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Fold R²: -0.0046443238333682135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Fold R²: 0.00782404296364747\n",
      "Best R²: 0.00782404296364747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define the number of splits for k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "best_model = None\n",
    "best_r2 = float('-inf')\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Define a new model for each fold\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(10, 1)),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate R² score\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    print(f\"Fold R²: {r2}\")\n",
    "    \n",
    "    # Update the best model if the current one is better\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best R²: {best_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a(x):\n",
    "    a.b = 5\n",
    "    def updateB():\n",
    "        a.b = 10\n",
    "\n",
    "    updateB()\n",
    "\n",
    "    return x + a.b\n",
    "a(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "TICKER = \"ETHUSDC\"\n",
    "STOP_DATE = pd.Timestamp(year=2025, month=1, day=1, hour=12)\n",
    "LONGEST_PERIOD_MONTHS = 12\n",
    "NUM_FOLDS = 4 # note: this will test NUM_FOLDS-1 times\n",
    "\n",
    "# optuna hyperparams\n",
    "def HYPERPARAMS(funct, **kwargs):\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
    "        epochs = trial.suggest_int(\"epochs\", 10, 50)\n",
    "        activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "        lstm_units = trial.suggest_int(\"lstm_units\", 32, 128)\n",
    "        \n",
    "        hyperparams = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs,\n",
    "            'lstm_units': lstm_units,\n",
    "            'activation': activation,\n",
    "        }\n",
    "        return funct(hyperparams=hyperparams, **kwargs)\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=== Q&A ===\n",
    "\n",
    "RL Model\n",
    "[      Train      ] [  Test  ]\n",
    "[  Train  ]  [Test]\n",
    "LSTM Model\n",
    "\n",
    "\n",
    "Q: Will this program only have access to \"train\" data? Should it have access to \"test\" data?\n",
    "A: It should not have access to \"test\" data. If we want to see how it performs on test data,\n",
    "we run a separate experiment from \"training\". Even automated performance monitoring on the\n",
    "\"test\" data will be conducted globally, not locally.\n",
    "\n",
    "Q: How will this program optimize the LSTM model before handing it off?\n",
    "Step 1: generate Walk-Forward Test Folds (A test B, AB test C, ABC test D, ... A-Y test Z)\n",
    "Step 2: use optuna to find the hyperparameters that have the best average MSE\n",
    "Step 3: train those hyperparameters on A-Z, publish\n",
    "\n",
    "Q: What financial indicators? What target?\n",
    "A: No financial indicators\n",
    "   Predicts \"Open\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "def getAssetData(ticker, maxPeriodLength_months=12):\n",
    "    df = pd.read_csv(f'assets/{ticker}.csv')\n",
    "\n",
    "    # filter to longest contiguous before STOP_DATE\n",
    "    df.drop(columns=[\"Ignore\", \"Close time\"], inplace=True)\n",
    "    df[\"Open time\"] = df[\"Open time\"].apply(lambda x: x * 1000 if len(str(x)) < len(\"1738369800000000\") else x)\n",
    "    df[\"Open time\"] = pd.to_datetime(df[\"Open time\"], unit='us')\n",
    "    df.set_index(\"Open time\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    df = df[df.index < STOP_DATE]\n",
    "\n",
    "    df[\"Gap\"] = df.index.to_series().diff().dt.total_seconds().div(60).fillna(0)\n",
    "    valid_intervals = df[df[\"Gap\"] <= 30]\n",
    "    last_gap_index = df[df[\"Gap\"] > 30].index[-1]\n",
    "    df = df.loc[last_gap_index:]\n",
    "    \n",
    "\n",
    "    df.drop(columns=[\"Gap\"], inplace=True)\n",
    "\n",
    "    # add financial indicators\n",
    "\n",
    "    # chop off N/A or longer than maxPeriodLength\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df.index > (df.index[-1] - pd.DateOffset(months=maxPeriodLength_months))]\n",
    "\n",
    "    # normalize except the target\n",
    "    target = df[\"Open\"].copy()\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    normalized_columns = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(normalized_columns, columns=df.columns, index=df.index)\n",
    "    df[\"Target\"] = target\n",
    "\n",
    "    return df\n",
    "\n",
    "def formatLSTMInput(df, numFolds=4, lookback=5):\n",
    "    foldEndIndices = [int(len(df) * (i + 1) / numFolds) for i in range(numFolds)]\n",
    "    foldEndIndices = [0] + foldEndIndices + [len(df)]\n",
    "\n",
    "    folds = [df.iloc[foldEndIndices[i]:foldEndIndices[i + 1]] for i in range(len(foldEndIndices) - 1)]\n",
    "    folds = [fold.reset_index(drop=True) for fold in folds]\n",
    "\n",
    "    def formatXy(fold):\n",
    "        if len(fold) <= lookback:\n",
    "            print(\"Your fold is smaller than the lookback+1 period, so bro, fix it, please\")\n",
    "            return None, None\n",
    "        X = pd.concat([fold.shift(i) for i in range(lookback, 0, -1)], axis=1).dropna()\n",
    "        X = np.reshape(X.values, (len(X), lookback, len(fold.columns)))\n",
    "        y = np.reshape(fold[\"Target\"].shift(-lookback).dropna(), (-1, 1))\n",
    "        return X, y\n",
    "\n",
    "    # format X and format Y\n",
    "    trainTestFolds = []\n",
    "    for i in range(len(folds)-2):\n",
    "        train_fold = pd.concat(folds[0:i + 1], axis=0)\n",
    "        test_fold = folds[i + 1]\n",
    "        X_train, y_train = formatXy(train_fold)\n",
    "        X_test, y_test = formatXy(test_fold)\n",
    "        trainTestFolds.append({\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "    \n",
    "    train_final = pd.concat(folds, axis=0).reset_index(drop=True)\n",
    "    X_final, y_final = formatXy(train_final)\n",
    "    finalData = {\n",
    "        'X_train': X_final,\n",
    "        'y_train': y_final,\n",
    "    }\n",
    "\n",
    "    return { 'train-test-folds': trainTestFolds, 'final-train': finalData }\n",
    "\n",
    "def generateModel(hyperparams):\n",
    "    # build the Keras sequential model\n",
    "    learning_rate = hyperparams.get('learning_rate', 0.001)\n",
    "    batch_size = hyperparams.get('batch_size', 32)\n",
    "    epochs = hyperparams.get('epochs', 20)\n",
    "    lstm_units = hyperparams.get('lstm_units', 50)\n",
    "    activation = hyperparams.get('activation', 'relu')\n",
    "    input_shape = hyperparams.get('input_shape', (10, 1))\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='relu', input_shape=input_shape),\n",
    "        Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def KFoldsTrainTest(hyperparams, trainTestFolds):\n",
    "    hyperparams = hyperparams.copy()\n",
    "    hyperparams['input_shape'] = trainTestFolds[0]['X_train'].shape[1:]\n",
    "\n",
    "    # train the model on each fold and evaluate\n",
    "    mse_scores = []\n",
    "    for fold in trainTestFolds:\n",
    "        model = generateModel(hyperparams)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        X_train, y_train = fold['X_train'], fold['y_train']\n",
    "        X_test, y_test = fold['X_test'], fold['y_test']\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(X_train, y_train, epochs=hyperparams['epochs'], batch_size=hyperparams['batch_size'], verbose=0)\n",
    "\n",
    "        # evaluate the model\n",
    "        mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # return the average MSE\n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "def getOptimalModel(saveAs):\n",
    "    data = getAssetData(TICKER, LONGEST_PERIOD_MONTHS)\n",
    "    inputs = formatLSTMInput(data, NUM_FOLDS)\n",
    "    \n",
    "    trainTestfolds = inputs['train-test-folds']\n",
    "    finalData = inputs['final-train']\n",
    "\n",
    "\n",
    "    objective = HYPERPARAMS(KFoldsTrainTest, trainTestFolds=trainTestfolds)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    bestHyperparams = study.best_trial.params\n",
    "\n",
    "    model = generateModel(bestHyperparams)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(finalData['X_train'], finalData['y_train'], epochs=bestHyperparams['epochs'], batch_size=bestHyperparams['batch_size'], verbose=1)\n",
    "    model.save(saveAs)\n",
    "    print(f\"Best hyperparameters: {bestHyperparams}\")\n",
    "\n",
    "    return { 'model': model, 'hyperparams': bestHyperparams }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 11:58:49,684] A new study created in memory with name: no-name-58aab716-9242-41d8-a551-83b6902b654f\n",
      "c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-28 11:59:21,071] Trial 0 finished with value: 609.3888956705729 and parameters: {'learning_rate': 0.000889053279249898, 'batch_size': 68, 'epochs': 37, 'activation': 'tanh', 'lstm_units': 95}. Best is trial 0 with value: 609.3888956705729.\n",
      "[I 2025-03-28 11:59:52,317] Trial 1 finished with value: 564.2031758626302 and parameters: {'learning_rate': 0.008266349674529452, 'batch_size': 124, 'epochs': 45, 'activation': 'relu', 'lstm_units': 108}. Best is trial 1 with value: 564.2031758626302.\n",
      "[I 2025-03-28 12:00:14,630] Trial 2 finished with value: 604.9088745117188 and parameters: {'learning_rate': 2.2394277173922455e-05, 'batch_size': 88, 'epochs': 16, 'activation': 'tanh', 'lstm_units': 113}. Best is trial 1 with value: 564.2031758626302.\n",
      "[I 2025-03-28 12:00:47,689] Trial 3 finished with value: 565.0469767252604 and parameters: {'learning_rate': 0.001907946204490494, 'batch_size': 94, 'epochs': 25, 'activation': 'tanh', 'lstm_units': 89}. Best is trial 1 with value: 564.2031758626302.\n",
      "[W 2025-03-28 12:01:24,802] Trial 4 failed with parameters: {'learning_rate': 0.00109368902603852, 'batch_size': 48, 'epochs': 36, 'activation': 'relu', 'lstm_units': 71} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ic2594\\AppData\\Local\\Temp\\ipykernel_1888\\3352699525.py\", line 25, in objective\n",
      "    return funct(hyperparams=hyperparams, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ic2594\\AppData\\Local\\Temp\\ipykernel_1888\\960354305.py\", line 130, in KFoldsTrainTest\n",
      "    model.fit(X_train, y_train, epochs=hyperparams['epochs'], batch_size=hyperparams['batch_size'], verbose=0)\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n",
      "    opt_outputs = multi_step_on_iterator(iterator)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 878, in _call\n",
      "    results = tracing_compilation.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 139, in call_function\n",
      "    return function._call_flat(  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 1322, in _call_flat\n",
      "    return self._inference_function.call_preflattened(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 216, in call_preflattened\n",
      "    flat_outputs = self.call_flat(*args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 251, in call_flat\n",
      "    outputs = self._bound_context.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1688, in call_function\n",
      "    outputs = execute.execute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ic2594\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-28 12:01:24,812] Trial 4 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "getOptimalModel(\"test-regression.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
